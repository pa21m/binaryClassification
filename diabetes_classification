# -*- coding: utf-8 -*-
"""AppliedAI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pjLr41c8g4yMjVpE6UNY1-NWD0QWyCk7
"""

!pip install -q kaggle

#used for uploading the kaggle api token to google colab
from google.colab import files
files.upload()

!mkdir ~/.kaggle/
!cp kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json

"""Downloading dataset from kaggle"""

!kaggle datasets download -d 'uciml/pima-indians-diabetes-database'

"""Importing required libraries"""

import pandas as pd
import  numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler 
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import precision_score, recall_score, f1_score

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

df = pd.read_csv('/content/pima-indians-diabetes-database.zip')#loads the pima indian dataset

df.head() #prints the first 5 entries of the dataframe

"""Data preprocessing"""

df.info() #used for checking null values and if the datatype matches with the features

columnsname = df.columns[1:8].tolist() #stores the column names in a list

"""**Reference 1**:Histogram Plot to visualize zero value count"""

#plots the histogram plot to find out the zero values for features that can't have zero value
for col in columnsname:
  sns.histplot(data= df, x = df[col])
  plt.show()

"""Replacing all the rows with zero values in Glucose,BloodPressure,	SkinThickness,Insulin,BMI,DiabetesPedigreeFunction,Age with the median of the row since they cannot have zero values

"""

#creates a copy of the dataframe and replaces al the zero value to the median of the column
df_modified = df.copy()
for col in columnsname:
  df_modified[col] = df_modified[col].replace(to_replace = 0, value = df_modified[col].median())

"""**Reference 2**:Correlation table"""

#creates the correalation table for each feature
corr = df_modified.corr()
corr.style.background_gradient(cmap='coolwarm')

df_modified['Outcome'].value_counts()#shows the total number of postive and negative outcomes in the training model

"""**Reference 3**:
Distribution plot of each feature according to the outcome 
"""

#creates a distribution plot for each feature according to their outcome
sns.displot(data = df_modified, x = 'Pregnancies', hue = "Outcome", kind='kde')
plt.show()

sns.displot(data = df_modified, x = 'Glucose', hue = "Outcome", kind='kde')
plt.show()

sns.displot(data = df_modified, x = 'BloodPressure', hue = "Outcome",kind ='kde')
plt.show()

sns.displot(data = df_modified, x = 'Insulin', hue = "Outcome",kind= 'kde')
plt.show()

sns.displot(data = df_modified, x = 'SkinThickness', hue = "Outcome", kind= 'kde')
plt.show()

sns.displot(data = df_modified, x = 'BMI', hue = "Outcome",kind = 'kde')
plt.show()

sns.displot(data = df_modified, x = 'Age', hue = "Outcome",kind = 'kde')
plt.show()

"""Splitting dataset into x (features) and y (outcome)"""

train_features = df_modified.iloc[:,:8] #x - patient features
train_target = df_modified["Outcome"] #y- target value

"""Feature scaling """

#standardizing the variables to put them on the same scale
scaler = StandardScaler()
scaler.fit(train_features)
scaled_features = scaler.transform(train_features) #transform data so that distribution has mean 0 and s.d 1

"""Converting scaled features into a dataframe"""

df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])
df_feat.head()

"""Splitting the dataset into train and test dataset with a 80:20 ratio"""

train_x, test_x, train_y, test_y = train_test_split(scaled_features, train_target, test_size= 0.2, random_state=25 ) #80:20 train-test ratio

#checking dataset sizes 
print("Size of Training data:{}".format(train_x.shape[0]))
print("Size of Training data:{}".format(train_y.shape[0]))
print("Size of Testing data:{}".format(test_x.shape[0]))
print("Size of Testing data:{}".format(test_y.shape[0]))

"""KNN model """

knn = KNeighborsClassifier(n_neighbors = 1) #using k-value = 1 
knn.fit(train_x, train_y) #fitting model to training dataset

"""Using predict method to predict values using  KNN model and test data

"""

knn_predict = knn.predict(test_x)

"""Confusion matrix"""

from sklearn.metrics import classification_report,confusion_matrix,ConfusionMatrixDisplay

print(confusion_matrix(test_y,knn_predict)) #create confusion matrix

print(classification_report(test_y,knn_predict)) #create classification report

"""Finding k-value """

error_rate = [] 
test_scores = []
train_scores = []

for i in range(1,20): #k-values from 1-20
    
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(train_x,train_y)
    predict = knn.predict(test_x)
    
    #add values to lists
    error_rate.append(np.mean(predict != test_y))
    train_scores.append(knn.score(train_x,train_y))
    test_scores.append(knn.score(test_x,test_y))

"""**Reference 4**:Error rate vs K-value graph"""

#Find k-value with lowest error rate
plt.figure(figsize=(12,8))
plt.plot(range(1,20),error_rate,color='blue', linestyle='solid', marker='o',markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K-Value')
plt.xlabel('K-Value')
plt.ylabel('Error Rate')

#score from testing datapoints used for training (train_x, train_y)
#get k-value with max train score
max_train_score = max(train_scores)
train_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]
print('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores))))

# score from testing datapoints used from testing dataset (test_x, test_y)
#get k-value with max test score
max_test_score = max(test_scores) 
test_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]
print('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))

"""Retrain with new K-value"""

knn = KNeighborsClassifier(n_neighbors=11) #using k = 11

knn.fit(train_x,train_y)

predict = knn.predict(test_x)

print('New value K=11')
print(confusion_matrix(test_y,predict))

print(classification_report(test_y,predict))

"""**Reference 5**:
Train and test score graph
"""

#plot graph to compare train and test scores
plt.figure(figsize=(20,8))
sns.lineplot(range(1,20),train_scores,marker='o',label='Train Score')
sns.lineplot(range(1,20),test_scores,marker='o',label='Test Score')
plt.title('Train Scores vs. Test Scores')

"""Final model"""

knn = KNeighborsClassifier(11)

knn.fit(train_x,train_y)
knn.score(test_x,test_y) #get accuracy score

"""Confusion matrix with final classifier """

predict_y = knn.predict(test_x)
confusion_matrix(test_y,predict_y)
pd.crosstab(test_y, predict_y, rownames=['True'], colnames=['Predicted'], margins=True)

from sklearn import metrics

conf_matrix = metrics.confusion_matrix(test_y, predict_y)
p = sns.heatmap(pd.DataFrame(conf_matrix), annot=True, cmap="viridis" ,fmt='g')
plt.title('KNN Confusion Matrix', y=1.1)
plt.ylabel('True label')
plt.xlabel('Predicted label')

"""Classification report"""

print(classification_report(test_y,predict_y))

"""Accuracy of KNN model"""

print("KNN model accuracy : {0:0.3f}%".format(metrics.accuracy_score(test_y, predict_y)*100))

"""Logistic Regression"""

logreg = LogisticRegression() #creates the LR model object    
logreg.fit(train_x, train_y) #calcluates the weights of each feature according the train dataset    
predict_log = logreg.predict(test_x) #Predicts outcome for the test dataset
con_log = confusion_matrix(test_y,predict_log, labels= logreg.classes_) #Creates confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix = con_log, display_labels= logreg.classes_)
disp.plot()#displays the confusion matrix plot
disp.ax_.set_title('Logistic Regression Confusion Matrix')
print(classification_report(test_y, predict_log))
accuracy_log = logreg.score(test_x,test_y) #calculates accuracy
print("Accuracy of the logistic regression model:{0:0.3f} %".format(accuracy_log*100))

"""Random Forest"""

randforest = RandomForestClassifier(n_estimators=100) #creates an object for the Ramdom forest model  
randforest.fit(train_x, train_y) # calculates weights for each feature according to the train dataset 
pred_rand = randforest.predict(test_x) #predicts the outcome for the test dataset   
con_rand = confusion_matrix(test_y, pred_rand, labels=randforest.classes_) #creates the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix = con_rand, display_labels= randforest.classes_)
disp.plot()#displays the confusion matrix plot
disp.ax_.set_title('Random Forest Confusion Matrix')
print(classification_report(test_y, pred_rand))
accuracy_rand = randforest.score(test_x, test_y)#calculates the accuracy
print("Accuracy of the random forest classifier model:{0:0.3f} %".format(accuracy_rand*100))
